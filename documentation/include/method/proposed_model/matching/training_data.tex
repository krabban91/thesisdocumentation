% !TEX root = ..\..\..\..\main.tex
\subsubsection{Training data}
\label{sec:method:proposed:matching:training}
The training data used to fit the classifier is mainly intended to have been categorized recently by a user. As mentioned in Section \ref{sec:method:proposed} the search space (the  unlabeled material) shrinks as the labeled set grows for every iteration. The labeled data is used to create a training set in order to fit the classifier and make new predictions. 
In the first iteration however, there is no labeled set to work with and therefore no training set for the classifier. When the classifier can not be fit the exploring of the search space is not possible. Instead of locking the workflow of the proposed model the matching module instead selects some material at random (without any predictions) to pass on to the relevance feedback module which creates some labeled data to use in future iterations.

In order to fit an \emph{SVM}\todo{check these emphasises} at least one relevant and one irrelevant data point is necessary and in order to fit a \emph{Deep SVM} it is necessary to have two relevant and two irrelevant data points, due to how it is trained (see Section \ref{sec:deepSVM}). When selecting material from the search space at random there is no guarantee that enough material is sampled in order to fit the classifier correctly within a certain number of iterations. 
To work around this issue the possibility to install an initially predefined training set was introduced. The initial training set is used in combination with the labeled data, that will slowly grow with every search iteration. A justification to add such a training set to the model is that in most cases when a person knows what to look for it has some previous knowledge of how such material would appear. When having a predefined training set with at least 2+2 relevant and irrelevant images, it is always possible to fit the classifier and the search space will be explored. \todokahl{Ofullst√§nding mening}Resulting in actual predictions and better odds of finding more relevant material to improve the training set even more.

Not having enough training data results in the incapability of fitting the classifier correctly, but having too much training data would force the classifier to take too much time to fit the best possible decision boundary for the data. Since the size of a search space could theoretically be infinitely large, so would the labeled set would also go towards infinity as the number of iterations increases. To prevent the labeled set to become to large an upper size limit was set to 500 data points. No evaluation or research was put into this number. Instead it was selected by the intuition of having a training set of that size (500 data points) it should be possible to present a small portion of material with some certainty. Having an upper limit of training material adds the possibility of two things: The time spent training the classifier is done in the same amount of time every iteration and by sampling a subset from a large labeled set allows the decision boundary to shift in-between iteration.

