% !TEX root = ..\..\..\..\main.tex
\subsubsection{Training data}
\label{sec:method:proposed:matching:training}
The training data used to fit the classifier is mainly intended to have been classified \todo{been categorized recently by the user.}by the user recently. As mentioned in Section \ref{sec:method:proposed} the search space (\todo{the } unlabeled material) shrinks as the labeled set grows for every iteration. The labeled data is used to create a training set in order to fit the classifier and make new predictions. 
In the first iteration however, there is no labeled set to work with and therefor no training set for the classifier. When the classifier can not be fit the exploring of the search space is not possible. Instead of locking the workflow of the proposed model the matching module instead \todo{selects some material at random}randomly selects some material (without any predictions) to pass on to the relevance feedback module and in the following iterations there will be some labeled data to use.

In order to fit an \emph{SVM}\todo{check these emphasises} at least one relevant and one irrelevant data point is necessary and in order to fit a \emph{Deep SVM} it is necessary to have two relevant and two irrelevant data points\todo{, due to how it is trained (see Section \ref{sec:deepSVM}).}. When selecting material from the search space at random there is no guarantee that enough material is sampled in order to fit the classifier correctly within a certain number of iterations. 
To work around this issue the possibility to install an initially \emph{predefined training set} was given\todo{created}. The initial training set is used in combination with the labeled data\todo{add ``,''} that will slowly grow with every search iteration. A justification to add such a training set to the model is that in most cases when a person knows what to look for it has some previous knowledge of how such material would appear. It would even be improbable that the user would not have some material at hand ready to be inserted into the model. When having a predefined training set with at least 2+2 relevant and irrelevant images, it is always possible to fit the classifier and the search space will be explored. \todokahl{Ofullst√§nding mening}Resulting in actual predictions and better odds of finding more relevant material to improve the training set even more.

Not having enough training data results in the incapability of fitting the classifier correctly, but having too much training data would \todo{force}make the classifier to take too much time to fit the best \todo{possible }decision boundary for the data. Since the size of a search space could infinitely large, so could labeled set be after a large number of iterations. To prevent the labeled set to become to large an upper size limit was set to 500 data points. No evaluation or research was put into this number. Instead it was selected by the intuition of having a training set of that size it should be possible to present a small portion of material with some certainty. Having an upper limit of training material adds the possibility of two things: The time spent training the classifier is done in the same amount of time every iteration and by sampling a subset from a large labeled set allows the decision boundary to shift in\todo{in-between} between iteration.   

