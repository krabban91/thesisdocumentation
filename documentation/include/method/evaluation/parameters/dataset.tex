% !TEX root = ..\..\..\..\main.tex
\subsubsection{Datasets for benchmark}
\label{sec:meth:eval:bench:dataset}

Since scenery datasets such as the dataset Places205, presented in Section \ref{sec:theory:dataset:places}, have a broad base of image material and have a large variety of material within the categories, Places205 is perfect for a parameter benchmark. However, due to the size of the dataset, the decision to only use a subset was made. Instead of using all 205 different scenery categories a subset of 23 classes was cherry-picked; all categories with a name starting with \emph{the letter B}. This gives a $\approx 4.3\%$ chance that a randomly selected image is relevant. But since only 25 images are presented per iteration using the entire subset as a search space would cause the number of iterations to be $\approx 11000$, the search space was reduced a bit further. The search space during the benchmark evaluations consists of 200 images of each category where one of the categories is marked as relevant and the others are not. 
The choice of just using a part of the dataset makes it hard to compare with other implementations of image recognition implemented on this dataset. As well as the fact that the algorithm is designed to be more lightweight and less time-consuming than the more usual approach of using large deep neural networks.

In Section \ref{sec:method:eval:param} an evaluation set is mentioned. The Evaluation set is constructed by using 50 images, that are not represented in the search space, of each category. Resulting in having an evaluation set of 1150 images with the same probability of randomly selecting a relevant image as in the search space.  

In addition to the 250 images of every category used in the search space and the evaluation set, 250 images of every category were sampled in order to have material for different predefined training sets. The different sizes of the predefined training sets vary and how they are evaluated is presented in Section \ref{sec:method:eval:param:training}. The different training sets are simply constructed to have an as broad base on irrelevant images -- sampling some images from all categories that are not relevant -- and the smaller sets of relevant images are subsets of the larger sets of relevant images.  

To not put all eggs in the same basket, the benchmarks have been performed with three different categories marked as relevant in three different evaluations. These three categories are \emph{Bar}, \emph{Baseball field} and \emph{Bedroom}. The three categories does not have very much in common but some of the other categories in the dataset might have some similarities. Having three different categories results in having three different search spaces, three different evaluation sets and three different setups of training sets. The data drawn from these categories will be presented in parallell. 

