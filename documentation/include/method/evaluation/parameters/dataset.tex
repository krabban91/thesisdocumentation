% !TEX root = ..\..\..\..\main.tex
\subsubsection{Datasets for benchmark}
\label{sec:meth:eval:bench:dataset}

Since scenery datasets such as the \todo{Places205}MIT Places205, presented in Section \ref{sec:theory:dataset:places}, have a broad base of image material and have a large variety of material within the categories, Places205 is perfect for a parameter benchmark. However, due to the size of the dataset, the decision to only use a subset \todo{remove ``of it''}of it was made. Instead of using all 205 different scenery categories a subset of 23 classes was cherry-picked\todo{wanted : ?, is cherry-picked a well known phrase?}: All categories with a name starting with \emph{the letter B}. This gives a $\approx 4.3\%$ chance that a randomly selected image is relevant. But since only 25 images are presented per iteration using the entire subset as a search space would cause the number of iterations to be $\approx 11000$, the search space was \todo{reduced a bit further}designed to a bit smaller. The search space during the benchmark\todo{benchmark evaluations}s consists of 200 images of each category where one of the categories is marked as relevant and the others are not. 
The choice of just using a part of the \todo{remove ``MIT places205''}MIT places205 dataset makes it hard to compare to\todo{with} other implementations of image recognition implemented on this dataset, \todo{. As well}as well as the fact that the algorithm\todo{the algorithm is designed}, which is designed to be more lightweight and less time-consuming then\todo{than} the more usual approach of using large deep neural networks. \todo{``in a world...'': I want to remove this sentence completely. /Gee}In a world where machines keeps getting better at replicating and overachieving in fields that humans used to dominate it is important to accept the shortcomings and bring forward the potential that computers have \cite{eetimes2015johnson}\cite{forbes2015thomsen}\cite{he2015delving}. 

In Section \ref{sec:method:eval:param} an evaluation set is mentioned. The Evaluation set is constructed by using 50 images, that are not represented in the search space, of each category. Resulting in having an evaluation set of 1150 images with the same probability of randomly selecting a relevant image as in the search space.  

In addition to the 250 images of every category used in the search space and the evaluation set, 250 images of every category were sampled in order to have material for different predefined training sets. The different sizes of the predefined training sets vary and how they are evaluated is presented in Section \ref{sec:method:eval:param:training}. The different training sets are simply constructed to have an as broad base on irrelevant images -- sampling some images from all categories that are not relevant -- and the smaller sets of relevant images are subsets of the larger sets of relevant images.  

To not put all eggs in the same basket, the benchmarks have been performed with three different categories marked as relevant in three different evaluations. These three categories are \emph{Bar}, \emph{Baseball field} and \emph{Bedroom}. The three categories does not have very much in common but some of the other categories in the dataset might have some similarities. Having three different categories results in having three different search spaces, three different evaluation sets and three different set \todo{setups}ups of training sets. The data drawn from these evalu\todo{categories}ations will be presented in parallell. 

