% !TEX root = ..\..\..\main.tex
\subsection{Parameter benchmarks}
\label{sec:method:eval:param}
\todo{}
The presentation of the proposed model in Section \ref{sec:method:proposed} covers the mostpart of how the optimal implementation is designed. What it does not cover is if some of the design decisions are good and not. \todo{For example in}In Section \ref{sec:method:proposed:rf} \todo{remove : ``that covers the relevance feedback module''}that covers the relevance feedback module The\todo{the} selection of images to present to the user is mentioned but not elaborated. \todokahl{In Section... Konstig mening. Omformulera.}In Section \ref{sec:method:proposed:matching:search}, that covers how the search space is explored, two stopping conditions are introduced\todo{and => , yet } and how they affect performance needs to be evaluated. \todo{replace sentence with: The five feature descriptors covered in Section \ref{sec:method:proposed:features}, need to be evaluated. Not only in combination but how they perform on their own.}In Section \ref{sec:method:proposed:features}, that covers the feature extraction module, five different feature descriptors are extracted to be used by the classifier. The usage of these feature descriptors alone needs to be compared to when they are combined. \todo{Entire sentence: As well as the effect of the training sets mentioned in Section \ref{sec:method:proposed:matching:training}. How much can the behaviour of the model change depending on training set sizes.}Finally the Section \ref{sec:method:proposed:matching:training}, the training data for the matching module, mentions having different training set sizes but not which sizes that are expected to work the best.
To summarize, the parameter \todo{benchmark will consist of => benchmarks are}benchmark will consist of the following four evaluations:
\begin{itemize}
\item The effect of presenting different sets of images to the user for relevance feedback (Section \ref{sec:method:eval:param:learning}).
\item The effect of proning\todo{pruning} the search space each\todo{each iteration=> during the search iterations} iteration (Section \ref{sec:method:eval:param:stopping}).
\item How well the deep SVM\todo{the ensemble of learners performs} behaves compared to its parts (Section \ref{sec:method:eval:param:features}).
\item The effect of using different kinds of training sets (Section \ref{sec:method:eval:param:training}).
\end{itemize}
\medskip

Before specifying which metrics that are intended to be used in these evaluation, the metrics need to be defined. The two more commonly used measures are \emph{recall}\todokahl{ekvationer del av meningen} (see Equation (\ref{eq:meth:recall}))\todo{Bake} and \emph{precison} (see Equation (\ref{eq:meth:precision}))\todo{Bake}. Often presented in pairs due to the fact that recall can reveal if the matching module presents too many false negatives while precision can indicate if too many false positives are presented. 

\begin{equation}
\label{eq:meth:recall}
\textnormal{recall} = \tfrac{\textnormal{True positives}}{\textnormal{True positives } + \textnormal{ False negatives}}
\end{equation}

\begin{equation}
\label{eq:meth:precision}
\textnormal{precision} = \tfrac{\textnormal{True positives}}{\textnormal{True positives } + \textnormal{ False positives}}.
\end{equation}

A metric that measures the models effectiveness is the \todo{remove so-called}so-called F1-measure\todo{Bake} \cite{powers2011evaluation}, seen in Equation (\ref{eq:meth:f1measure}). The F1-measure is the harmonic mean of recall and precision\todo{cite here instead.}, meaning that recall and precision are equally weighted.\todo{and that smaller values are punished more than when using a normal average function.} 

\begin{equation}
\label{eq:meth:f1measure}
\begin{split}
\textnormal{F1-measure} &= 2 \tfrac{precision \times recall}{precision + recall}\\
&= \tfrac{\textnormal{True positives}}{\textnormal{True positives} +\frac{\textnormal{False positives}+\textnormal{False negatives}}{2}},
\end{split}
\end{equation}
The final metric to present is the accuracy of the model (see Equation (\ref{eq:meth:accuracy}))\todo{bake}. Simply put the rate of correct predictions that are made. 

\begin{equation}
\label{eq:meth:accuracy}
\textnormal{accuracy} = \tfrac{\textnormal{True positives} + \textnormal{True negatives}}{\textnormal{True positives} + \textnormal{True negatives}+ \textnormal{False positives} + \textnormal{False negatives}}.
\end{equation}


Apart from the \todo{these metrics}metrics the evaluations include how long the calculations of an iteration takes, how many images that are passed through the classifier in the matching module and how many of the presented images that are relevant.   

In order to make sure that the search space is explored and categorized correctly and at the same time see how well the model performs at classification, two different methods are used to calculate performance. \todo{In addition to the search space an evaluation set, in complete disjunction to the search space, is used. }To do this an evaluation set is used and the evaluation set is completely disjunct from the search space. The datasets for the benchmarks and how the are constructed is presented in Section \ref{sec:meth:eval:bench:dataset}. 

The benchmark evaluations are measured in two different ways.
\begin{enumerate}
\item How well the model classifies an evaluation set each iteration. For the evaluation set the following metrics are used: 
	\begin{itemize}
		\item Recall
		\item Precision
		\item F1-Measure
		\item Accuracy
	\end{itemize}   
\item How well the model classifies the images that are presented to the user each iteration. For the search space the following metrics are used:
	\begin{itemize}
		\item Accumulated recall 
		\item Accumulated precision
		\item Accumulated F1-Measure
		\item Accumulated accuracy
		\item Retrieved \todo{relevant}images
		\item Handled images
		\item Time taken calculating
	\end{itemize}   
\end{enumerate}
\medskip
When measuring the performance of classifying the search space the metrics are the accumulated value of the performance so far during the search, meaning that after iteration 20 the performance is measured on the 500 images that have recieved an prediction by the proposed model.

In order to retrieve a general trend, each evaluation setting is run 5 separate times and the metrics during these runs are presented with the maximum, the minimum and the geometric mean of each iteration. This allows the graphs that are presented in this section to show how much the metrics varied depending on different factors, such as image selection or how the decision boundary was fitted.

In all benchmarks, except for the one in Section \ref{sec:method:eval:param:features} where the ensemble is evaluated against its parts, the classifier is implemented as proposed in Section \ref{sec:method:proposed}\todo{classifier section.} and will have use all five feature descriptors. In addition; all benchmarks, with the exception of the one described in Section \ref{sec:method:eval:param:training} where training data is evaluated, the matching model will have a predefined training set of 5 relevant images and 50 non-relevant.

The evaluation of the model is intended to be fair and the performance to be measured in an as general way as possible but still possible to perform within a relatively small time frame. The datasets for the evaluation are therefore constructed specifically for this purpose. 

\input{include/method/evaluation/parameters/dataset}
\input{include/method/evaluation/parameters/learning}
\input{include/method/evaluation/parameters/stopping}
\input{include/method/evaluation/parameters/features}
\input{include/method/evaluation/parameters/training}
