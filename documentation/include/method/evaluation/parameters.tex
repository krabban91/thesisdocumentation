% !TEX root = ..\..\..\main.tex
\subsection{Parameter benchmarks}
\label{sec:method:eval:param}
The presentation of the proposed model in Section \ref{sec:method:proposed} covers the most part of how the optimal implementation is designed. What it does not cover is if some of the design decisions are good or not. For example Section \ref{sec:method:proposed:rf} the selection of images to present to the user is mentioned but not elaborated. There are two stopping conditions introduced in Section \ref{sec:method:proposed:matching:search} which need to be evaluated to see how they effect the performance. The five feature descriptors covered in Section \ref{sec:method:proposed:features}, need to be evaluated. Not only in combination but how they perform on their own.
%In Section \ref{sec:method:proposed:features}, that covers the feature extraction module, five different feature descriptors are extracted to be used by the classifier. 
As well as the effect of the training sets mentioned in Section \ref{sec:method:proposed:matching:training}. How much can the behavior of the model change depending on training set sizes.
To summarize, the parameter benchmarks are the following four evaluations:
\begin{itemize}
\item The effect of presenting different sets of images to the user for relevance feedback (Section \ref{sec:method:eval:param:learning}).
\item The effect of pruning the search space during the search iterations (Section \ref{sec:method:eval:param:stopping}).
\item How well the ensemble of learners performs compared to its parts (Section \ref{sec:method:eval:param:features}).
\item The effect of using different kinds of training sets (Section \ref{sec:method:eval:param:training}).
\end{itemize}
\medskip

Before specifying which metrics that are intended to be used in these evaluations, the metrics need to be defined. The two more commonly used measures are (\ref{eq:meth:recall}) and (\ref{eq:meth:precision}).

\begin{equation}
\label{eq:meth:recall}
\textnormal{recall} = \tfrac{\textnormal{True positives}}{\textnormal{True positives } + \textnormal{ False negatives}}
\end{equation}

\begin{equation}
\label{eq:meth:precision}
\textnormal{precision} = \tfrac{\textnormal{True positives}}{\textnormal{True positives } + \textnormal{ False positives}}.
\end{equation}
Often presented in pairs due to the fact that recall can reveal if the matching module presents too many false negatives while precision can indicate if too many false positives are presented. 

A metric that measures the models effectiveness is the F1-measure (\ref{eq:meth:f1measure})

\begin{equation}
\label{eq:meth:f1measure}
\begin{split}
\textnormal{F1-measure} &= 2 \tfrac{precision \times recall}{precision + recall}\\
&= \tfrac{\textnormal{True positives}}{\textnormal{True positives} +\frac{\textnormal{False positives}+\textnormal{False negatives}}{2}}.
\end{split}
\end{equation}

The F1-measure is the harmonic mean of recall and precision \cite{powers2011evaluation}, meaning that recall and precision are equally weighted and that smaller values are punished more than when using a normal average function. 

The final metric to present is the accuracy (\ref{eq:meth:accuracy}) 

\begin{equation}
\label{eq:meth:accuracy}
\textnormal{accuracy} = \tfrac{\textnormal{True positives} + \textnormal{True negatives}}{\textnormal{True positives} + \textnormal{True negatives}+ \textnormal{False positives} + \textnormal{False negatives}},
\end{equation}
which simply put is the total rate of correct predictions. 


Apart from these metrics the evaluations include how long the calculations of an iteration takes, how many images that are passed through the classifier in the matching module and how many of the presented images that are relevant.   

In order to make sure that the search space is explored and categorized correctly and at the same time see how well the model performs at classification, two different methods are used to calculate performance. In addition to the search space an evaluation set, in complete disjunction to the search space, is used. The datasets for the benchmarks and how they are constructed is presented in Section \ref{sec:meth:eval:bench:dataset}. 

The benchmark evaluations are measured in two different ways.
\begin{enumerate}
\item How well the model classifies an evaluation set each iteration. For the evaluation set the following metrics are used: 
	\begin{itemize}
		\item Recall
		\item Precision
		\item F1-Measure
		\item Accuracy
	\end{itemize}   
\item How well the model classifies the images that are presented to the user each iteration. For the search space the following metrics are used:
	\begin{itemize}
		\item Accumulated recall 
		\item Accumulated precision
		\item Accumulated F1-Measure
		\item Accumulated accuracy
		\item Retrieved relevant images
		\item Handled images
		\item Time taken calculating
	\end{itemize}   
\end{enumerate}
\medskip
When measuring the performance of classifying the search space the metrics are the accumulated value of the performance so far during the search. This mean that after iteration 20 the performance is measured on the 500 images that have received an prediction by the proposed.

In order to retrieve a general trend, each evaluation setting is run five separate times and the metrics during these runs are presented with the maximum, the minimum and the geometric mean of each iteration. This allows the graphs that are presented in this section to show how much the metrics varied depending on different factors, such as image selection or how the decision boundary was fitted.

In all benchmarks, except for the one in Section \ref{sec:method:eval:param:features} where the ensemble is evaluated against its parts, the classifier is implemented as proposed in Section \ref{sec:method:proposed:matching:classifier} and will have use all five feature descriptors. In addition; all benchmarks, with the exception of the one described in Section \ref{sec:method:eval:param:training} where training data is evaluated, the matching model will have a predefined training set of 5 relevant images and 50 non-relevant.

The evaluation of the model is intended to be fair and the performance to be measured in an as general way as possible but still possible to perform within a relatively small time frame. The datasets for the evaluation are therefore constructed specifically for this purpose. 

\input{include/method/evaluation/parameters/dataset}
\input{include/method/evaluation/parameters/learning}
\input{include/method/evaluation/parameters/stopping}
\input{include/method/evaluation/parameters/features}
\input{include/method/evaluation/parameters/training}
