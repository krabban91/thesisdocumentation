% CREATED BY DAVID FRISK, 2016
% !TEX root = ..\main.tex

\chapter{Theory}
\label{chapter:cbirtheory}
In this chapter the theory of the key concepts behind this thesis are presented. Here content-based image retrieval is explained as well as the difficulty of closing the semantic gap. The idea of relevance feedback is introduced as it is a key module in the thesis. Lastly the different datasets used in evaluations are presented accompanied with a brief introduction to how images are stored digitally. 
The chapters \emph{\nameref{chapter:imagetheory}} and \emph{\nameref{chapter:mltheory}} extends theory in their respective branches.

\section{Content-based image retrieval}

Content-based image retrieval (CBIR) is a term referring to techniques used in computer vision, where the goal is to find images with similarities in large databases. Given a query that is presented to the system, the output would be a set of images extracted from the total set which have the highest resemblance to the query. Content-based refers to the information stored in the image itself and not, as is the case in tag-based image retrieval (TBIR), the metadata of the image. CBIR is therefore a viable approach if either the metadata is non-existent or the classification is of some other variety than what the metadata can give. A situation when CBIR might be good to use is when the visual content of the semantic nature or there are reoccurring objects in several different images. In modern CBIR systems there are four reoccurring major parts: Feature extraction, where the raw features are recovered. Feature reduction, the recovered features are used to reduce feature dimensionality and storage space usage. Ranking, systemise the images so that the system can categorize the images in the dataset depending on resemblance to the query. Finally relevance feedback, the final feedback given by an expert user if needed correcting the algorithms predictions \cite{kundu2015graph}. A large problem in image retrieval is that the query image might hold information easily perceived by a user but hard to concretize in pixel data and features, this problem is called the semantic gap. It can be said to be the difference that arises when two different linguistic representations try to describe the same object. This is a relevant issue whenever the perspective of a human is tried to be represented by a computer. The semantic gap is mounted by some function that can from the pixels representing an image make a computer understand and recognize what object that it is percieving \cite{smeulders2000semantic}.


\section{Relevance feedback}
\label{sec:theory:relfeed}
A way to avoid the problems that arise when dealing with the semantic gap is to use relevance feedback. 
Relevance feedback can be said to be the direct interaction between a user and a machine in learning. 
The user reviews and corrects the predictions that the machine has made. The machine can in return use this information to re-evaluate the predictions that were made.
The process is in general that a user is presented with a number of images by a machine learning algorithm. Images that the algorithm has tried to label with the help, or occlusion, of some pre-training. These images are re-evaluated by the user and corrected by her if the corresponding label happens to be falsely assigned and acknowledged otherwise. With these adjustments to the data the to the data the retrieval process is refined in an attempt to make future classifications better. These two parts are then iteratively carried out as the algorithm keeps searching through the dataset for the required images \cite{IRJET2017relevancefeedback}. There are different kinds of relevance feedback methods, the three most common are explicit, implicit and blind (pseudo) feedback. Using explicit feedback means that a user, knowingly of that her actions will affect how future material will be presented, indicates the relevance of the material presented to it. The first of the other two feedback variations is implicit which either means that the users behavior is observed or that the user is unknowing that the feedback are used as relevance feedback for the system. The other is pseudo relevance feedback which is a form of automated feedback that uses the first query as relevant results. In view of the situation of investigations having a need for the user to review all images in any case, explicit feedback is the best viable option. The explicit feedback is used to create a continuous data confirmation and thus creating more reliable data in each iteration.


\section{Image formats}
\label{sec:theory:image_formats}
There are several file formats available and different ways to store images, such as storing images as as uncompressed and compressed raster formats as well as vector formats. When an image is stored with a raster format it is represented by a grid of pixels with a depth depending on the information of the image. The most common way to store image information is to use a 24-bit RGB pixel, where RBG is an abbrevation for red green and blue. Each 24-bit pixel has three equally sized channels of 8 bits, which makes each color channel range between 0 and 255. Resulting in approximately 16.8 million different combinations for colors, where a human can perceive about 10 million \cite[p.388]{judd1975color}. A computer screen usually operates at the described color setting. The size of the file simply depends on how many of these pixels that are stored, i.e. the size directly depends the dimensions (width and height) of the image. As the sizes may vary, some images can become expensive to process. Because of this it can be prudent to downsize the image to a smaller number of pixels and thus avoiding unnecessarily large amounts of data. Downsizing can be especially useful since high pixel information is not always equivalent to good performance \cite{torralba200880}.
There are several different color spaces and of which some are used in image analysis depending on the task. The most commonly known being RGB which is a color space that generated based on how colors are created based on mixing light. The use of additive color combinations of red green and blue became useful when the production television sets and computer screens rose, the different channels of RGB are visualised in Figure \ref{fig:theory:colors:rgb}. Though these three color channels are useful as lightsources, they are considered rather inept in image analysis. Humans tend to react more on the hue and saturation of an image than these color channels \cite{alzu2015semantic}\cite{cheng2001color}. 


\fourfigure
{figure/colors/cat.jpg}
{figure/colors/rgb/r.png}
{figure/colors/rgb/g.png}
{figure/colors/rgb/b.png}
{The different channels of the color space RGB. The left image is the original image and the other three show each independent channel From the left: Red, green and blue channels. The brighter the pixel the higher color value since RGB is an additive color space.}
{fig:theory:colors:rgb} 

\subsection{Hue, saturation, value}
\label{sec:HSV_theory}
Hue, saturation and value, abbreviated as HSV, is a color space in line with RGB. The difference is that HSV is a cylindrical representation of RGB, where RGB is mapped as a cube where the channels r, g and b can be interpreted as the often named x, y and z axes. The HSV can be mapped cylindrically where hue is the degree position of the cylinder, saturation is the radius and value is the height. HSV is considered to be a color space that is a closer representation how a human perceives the world and thus also often used in the field of CBIR and image analysis as a whole. The different color channels can be seen in Figure \ref{fig:theory:colors:hsv}.


\fourfigure
{figure/colors/cat.jpg}
{figure/colors/hsv/h.png}
{figure/colors/hsv/s.png}
{figure/colors/hsv/v.png}
{The different channels of the color space HSV. From the left: All color channels are active, only the hue channel, only the saturation channel and only the value channel.}
{fig:theory:colors:hsv} 

\subsection{YCbCr}
\label{sec:ycbcr}
This representation is composed to work towards human perception where the luminance component (Y) can be seen as analogous to the brightness or light component and the two chroma (Cb and Cr) filling out the color spectra \cite{midha2014analysis}. The three channels can be seen in Figure \ref{fig:theory:colors:ycbcr}.
The color information is not always as vital for human perception as the brightness is. The human retina has three types of photoreceptor cells where two are commonly referred to: The rods, that are very sensitive to light and can be triggered by a single proton, and the cones, that are less sensitive to light but reacts to differently to different wavelengths of light. A human has $\approx120$ million rods and $\approx6$ million cones. Then number of photoreceptor is somewhat of an indicator of which channel in YCbCr that affects human perception the most. 
The luminance is often used in edge detection since it conveys textures, illuminates the shapes of objects and portrays depth in images \cite{su2011coldimage}\cite{prajapatiedge}. 

\fourfigure
{figure/colors/cat.jpg}
{figure/colors/ycbcr/y.png}
{figure/colors/ycbcr/cb.png}
{figure/colors/ycbcr/cr.png}
{The different channels of the color space YCbCr. From the left: full image, the luma channel (Y) and the two chroma channels (Cb) and (Cr) in that order. In the luma channel edges are abstracted from the color compositions.}
{fig:theory:colors:ycbcr} 


\section{Datasets}
\label{sec:theory:datasets}
Datasets are used to evaluate and compare the performance of a proposed model with other studies. This is often done for CBIR systems as well as classification systems.
Examples of evaluations that uses datasets are plain recognition, image retrieval and image classification. 
As a first means of both evaluation and with reference to several recent papers (\cite{wang2001simplicity}, \cite{subrahmanyam2013modified}, \cite{nagaraja2015low} and \cite{elalami2014new}) focus one of the datasets used in the thesis is the dataset Corel-1000. 
This set comes with its limitations as it is relatively small in comparison with the huge datasets used to train deep neural networks. Neural networks such as GoogLeNet \cite{szegedy2015going}, AlexNet \cite{krizhevsky2012imagenet} and VGG-16 \cite{simonyan2014very} are designed to compete in the ImageNet Large Scale Visual Recognition Competition (ILSVRC) \cite{russakovsky2015imagenet}, a yearly object detection contest where 1000 object categories are present. Since the goal of the thesis is to learn general concepts and not to detect objects another dataset was used: The dataset Places205, designed by MIT, described in Section \ref{sec:theory:dataset:places}.   


\subsection{Corel-1000}
\label{sec:corel}
The Corel set is an image dataset often cited and used in validation of different CBIR systems \cite{coreldataset2017}. The dataset is a low resolution set composed of 80 classes (concepts) with 10.800 images in total. Due to the size of the dataset a subset, called the Corel-1000 dataset, is used to compare the proposed method with related CBIR approaches \cite{wang2001simplicity}. The Corel-1000 dataset is a subset of the Corel dataset which contains 1000 images, composed by 10 classes with 100 images in each class. The images in this dataset are of the sizes $64\times96$ and $96\times64$ pixels depending on their orientation. The 10 classes are referred to as Africans, Beaches, Buildings, Buses, Dinosaurs, Elephants, Flowers, Horses, Mountains and Food. 

\subsection{Places205}
\label{sec:theory:dataset:places}
Places205 is a dataset produces by MIT and collaborators in the search for ever better human-reaching performance with machine-learning. 

The MIT places205 is chosen to be part of the evaluation of the algorithm presented in this thesis since it has a large variety of images and classes and is a well-known dataset \cite{zhou2016places}. It is a repository of millions scene photographs, labeled with scene semantic categories and attributes. The dataset consists of 205 sceneries with an average of 12000 images in each class. 
The different sceneries of the dataset places205 can be seen in Table \ref{table:appendix:dataset:places} in the appendices. The table lists all the names of the categories. 

