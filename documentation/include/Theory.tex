% CREATED BY DAVID FRISK, 2016
% !TEX root = ..\main.tex

\chapter{Theory}
\label{chapter:cbirtheory}
In this section the theory of the key concepts behind this thesis will be presented. Here content-based image retrieval is explained as well as the difficulty of closing the semantic gap. The idea of relevance feedback is introduced as it is in the center of this project. Lastly the different datasets used in this thesis are presented accompanied with a brief introduction to digital images. 
The chapters \nameref{chapter:imagetheory} and \nameref{chapter:mltheory} extends the information in their respective branches.
\todo{might need a better flow}

\section{Content-based image retrieval}

Content-based image retrieval (CBIR) is a term referring to techniques used in computer vision with the focus of searching for images in large databases.\todo{comment on the todo (1). Think it might work as is.}\todo{(1)maybe use , and rephrase?} Given a query that is presented to the system, the output would be a set of images extracted from the total set which have the highest resemblance to the query. Content-based refers to the information stored in the image itself and not, as is the case in tag-based image retrieval (TBIR), the metadata of the image. CBIR is therefore a viable approach if either the metadata is non-existent or the classification is of some other variety than what the metadata can give. A situation when CBIR might be good to use is when the visual content of the semantic nature or there are reoccurring objects in several different images. In modern CBIR systems there are four reoccurring major parts: Feature extraction, where the raw features are recovered. Feature reduction, the recovered features are used to reduce feature dimensionality and storage space usage. Ranking, systemise the images so that the system can categorize the images in the dataset depending on resemblance to the query. Finally relevance feedback, the final feedback given by an expert user if needed correcting the algorithms predictions \cite{kundu2015graph}. A large problem in image retrieval is that the query image might hold information easily perceived by a user but hard to concretize in pixel data and features, this problem is called the semantic gap. It can be said to be the difference that arises when two different linguistic representations try to describe the same object. This is a relevant issue whenever the perspective of a human is tried to be represented by a computer. Simply put, how from the pixels representing an image will the computer understand and recognize what object it is perceiving \cite{smeulders2000semantic} \todokahl{Simply put... konstig mening. Omformulera.}\todo{The entire sentence is strange. Something is missing.}. 


\section{Relevance feedback}
\label{sec:theory:relfeed}
A way to avoid the problems that arise when dealing with the semantic gap is to use relevance feedback. Relevance feedback can be said to be the direct interaction between a user and a machine in learning. Here the user reviews and corrects the estimations of the \todo{Meningen behöver delas upp och saknar bisatser.}machine and ranks these depending on the nature of the dataset and sends this feedback back to the machine for re-evaluation and training. The process is in general that a user is presented with a number of images by a machine learning algorithm. Images that the algorithm has tried to label with the help, or occlusion, of some pre-training. These images are re-evaluated by the user and corrected by her if the corresponding label happens to be falsely assigned and acknowledged otherwise. With these adjustments to the data the to the data the retrieval process is refined in an attempt to make future classifications better. These two parts are then iteratively carried out as the algorithm keeps searching through the dataset for the required images \cite{IRJET2017relevancefeedback}. There are different kinds of relevance feedback methods, the three most common are \emph{explicit}, \emph{implicit} and \emph{blind (pseudo) feedback}. Using explicit feedback means that a user, knowingly of that her actions will affect how future material will be presented, indicates the relevance of the material presented to it. The first of the other two feedback variations is implicit which either means that the users behavior is observed or that the user is unknowing that the feedback are used as relevance feedback for the system. The other is pseudo relevance feedback which is a form of automated feedback that uses the first query as relevant results. In view of the situation of investigations having a need for the user to review all images in any case, explicit feedback is the best viable option. The explicit feedback is used to create a continuous data confirmation and thus creating more reliable data in each iteration.


\section{Image formats}
\label{sec:theory:image_formats}
There are several file formats available and different ways to store images, such as storing images as as uncompressed and compressed raster formats as well as vector formats. When an image is stored with a raster format it is represented by a grid of pixels with a depth depending on the information of the image. A common type is a 24-bit RGB pixel, where RBG is an abbrevation for red green and blue. Each 24-bit pixel has three equally sized channels of 8 bits, which makes each color channel range between 0 and 255. Resulting in approximately 16.8 million different combinations for colors, where a human can perceive about 10 million \cite[p.388]{judd1975color}. A computer screen usually operates at the described color setting. The size of the file simply depends on how many of these pixels that are stored, i.e. the size directly depends the dimensions (width and height) of the image. As the sizes may vary, some images can become expensive to process. Because of this it can be prudent to downsize the image to a smaller number of pixels and thus avoiding unnecessarily large amounts of data. Downsizing can be especially useful since high pixel information is not always equivalent to good performance \cite{torralba200880}.
There are several different color spaces and of which some are used in image analysis depending on the task. The most commonly known being RGB which is a color space that generated based on how colors are created based on mixing light. The use of additive color combinations of red green and blue became useful when the production television sets and computer screens rose. Though these three color channels are useful as lightsources, they are considered rather inept in image analysis. Humans tend to react more on the hue and saturation of an image than these color channels \cite{alzu2015semantic}\cite{cheng2001color}. 


\subsection{Hue, saturation, value}
\label{sec:HSV_theory}
\emph{Hue}, \emph{saturation} and \emph{value}, abbreviated as HSV, is a color space in line with RGB. The difference is that HSV is a cylindrical representation of RGB, where RGB is mapped as a cube where the channels r, g and b can be interpreted as the often named x, y and z axes. The HSV can be mapped cylindrically where hue is the degree position of the cylinder, saturation is the radius and value is the height. HSV is considered to be a color space that is a closer representation how a human perceives the world and thus also often used in the field of CBIR and image analysis as a whole. The object are presented through relevance feedback to the algorithm which is done by a human to give credit to why one would use HSV \todo{This sentence. What?}. One reason is that to have components that can described in some form of light, value (V) for HSV and luminance (Y) for YCbCr, component is separated from the color component(s) prominent in some color spaces, as the RGB color space where each channel have light built into it, which gives a more noisy interpretation of the light than if the channel were to be separated \todo{The entire sentence (starting at one reason): why is it here? Move to image format or remove.}. 

\subsection{YCbCr}
\label{sec:ycbcr}
YCbCr is a color space used mainly in color image pipeline \todo{Sentence. Meaning?} for video and photography systems. The color space was invented during development of a digital video standard and set to lessen the total bandwidth required \todo{required total bandwidth}. This representation is composed to work towards human perception where the luminance component (Y) can be seen as analogous to the brightness or light component and the two chroma (Cb and Cr) filling out the color spectra \cite{midha2014analysis}. The color information is not always as vital for human perception as the brightness where a human has $\approx120$ million rods which are sensitive to light and the achromatic part of the vision, while the less light prone cones \todo{What does this mean? The color experience is derived from cones. } are of the magnitude 6 million which constitutes color vision \todo{The focus of entire sentence is off. Split up.}. The use of luminance is used in edge detection since luminance conveys texture and shape of an object \cite{su2011coldimage}\cite{prajapatiedge} \todo{Sentence does not say anything. Use the cites to make a point.}\todo{change to make it work}. 


\section{Datasets}
\label{sec:theory:datasets}
Different datasets are used as means to evaluate the performance of different CBIR systems \todo{as well as classification systems. So CBIR systems does not cover it.}. It \todo{It? Evaluation?} can be \emph{plain recognition}, \emph{image retrieval} or \emph{image classification}. Of note is the massive amounts of different datasets that has come into existence in later years in the search for ever efficient systems and algorithms to meet the growing demand of big data handling and new harder sets as the work goes faster and faster in developing fields \todo{long sentence. Does not paint anything.} . As a first means of both evaluation and with reference to several recent papers \cite{liu2015content}\cite{wang2015new} \todo{why not one of the papers compared with?} focus one of the datasets chosen \todo{Chosen? No. Not in theory. } is Corel-1000 \cite{alzu2015semantic}. This set comes with its limitations as it is relatively small in comparison with most datasets used for the more \todo{More efficient? Not really.  Different areas.} efficient methods already recognized in the field of image analysis such as the Deep neural networks  GoogLeNet, AlexNet and VGG's \todo{Emphesize and cite. Or omit. } different nets. In the ImageNet challenge \todo{Name it? Which of all?} the networks are to differentiate and estimate a couple of hundred different classes. This is not of interest \todo{Yes it is. But the entire paragraph is unnecessary.} in this thesis since there is only focus on if an image has some malicious content, thus being relevant, or not malicious content, hence non-relevant \todo{Entire sentence is F-d.}. With this in mind a varied set to evaluate against is essential to get a more correct estimation of the algorithms \todo{The sentence is not very theory-y} performance. In this regard MIT's Places205 dataset is used \todokahl{Ge framåtreferent att det ska beskrivas senare.}\todo{This is only reason enough to use Places. Not omitting ILSVRC dataset}.\todo{flow and relevance need to be worked on}

\subsection{Corel-1000}
\label{sec:corel}
The Corel set is an image dataset often cited and used in validation of different CBIR systems \cite{coreldataset2017}. The dataset is a low resolution set composed of 80 classes (concepts) with 10.800 images in total. Due to the size of the dataset a subset, called the Corel-1000 dataset, is used to compare the proposed method with related CBIR approaches \cite{wang2001simplicity}. The Corel-1000 dataset is a subset of the Corel dataset which contains 1000 images, composed by 10 classes with 100 images in each class. The images in this dataset are of the sizes $64*96$ and $96*64$ pixels depending on their orientation. The 10 classes are referred to as \emph{Africans}, \emph{Beaches}, \emph{Buildings}, \emph{Buses}, \emph{Dinosaurs}, \emph{Elephants}, \emph{Flowers}, \emph{Horses}, \emph{Mountains} and \emph{Food}. 

\subsection{Places205}
\label{sec:theory:dataset:places}
Places205 is a dataset produces by MIT and collaborators in the search for ever better human-reaching performance with machine-learning. 

The MIT places205 is chosen to be part of the evaluation of the algorithm presented in this thesis since it has a large variety of images and classes and is a well-known dataset \cite{zhou2016places}. It is a repository of millions scene photographs, labeled with scene semantic categories and attributes. The dataset consists of 205 sceneries with an average of 12.000 images in each class. 
The different sceneries of the dataset places205 can be seen in Table \ref{table:appendix:dataset:places} in the appendices. The table lists all the names of the categories. 




