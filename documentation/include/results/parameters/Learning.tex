% !TEX root = ..\..\..\main.tex

\subsection{Classifier learning method}
\label{sec:res:learning}

The entirety of this evaluation is introduced in Section \ref{sec:method:eval:param:learning} and the results of the different settings are presented here. 
The four different settings of the evaluation are to present the \textbf{Top20+Bottom5}, \textbf{Top25}, \textbf{Top20+Middle5} and \textbf{Top5+Bottom20}.

As mentioned in Section \ref{sec:method:eval:param} the performance of the model is measured in two ways. The performance on an evaluation set as well the performance over the entire search space.

\subsubsection{Evaluation set}
\label{sec:res:learning:eval}
When classifying the evaluation set each iteration the measurements of the settings ended up to be very similar. The performance of the first three settings were almost identical. An initial performance peak in classifying the set that later on dropped off. While the performance of the fourth setting deviated from the performance of the others the measurements were about the same throughout all the iterations.

Independently of how images were selected the recall (see Figure \ref{fig:learning:eval_set:recall}) of the settings only differed marginally. With an exception of the fourth setting that in some manner deviated from the other ones. 
The precision of the fourth setting deviated from the other settings, which can be seen in Figure \ref{fig:learning:eval_set:precision}. Due to the low precision of the fourth setting, the F1-measure is low as well, which can be seen in Figure \ref{fig:learning:eval_set:f1}. The cause of this is that the fourth setting had an higher number of false positives than the other settings had.

An important observation to make is that towards the end of the evaluations all four settings have the same training data and therefor classifies the evaluation set accordingly. Having the complete training set results in an F1-measure around 0.32 when retrieving the category \emph{Bar} and around 0.65 when retrieving the other two from the remaining 22 categories in the evaluation set.
\tripfigurenear
{include/graphs/learning/baseball_field/eval_set/precision_extra.png}
{include/graphs/learning/bedroom/eval_set/precision_extra.png}
{include/graphs/learning/bar/eval_set/precision_extra.png}
{The precison that the different settings had on the evaluation set for the three different category searches. Note that the setting \emph{Top5+Bottom20} deviates from the other settings.}
{fig:learning:eval_set:precision}

\tripfigurenear
{include/graphs/learning/baseball_field/eval_set/recall_extra.PNG}
{include/graphs/learning/bedroom/eval_set/recall_extra.PNG}
{include/graphs/learning/bar/eval_set/recall_extra.PNG}
{The recall rate on the the evaluation set.}
{fig:learning:eval_set:recall}

\tripfigurenear
{include/graphs/learning/baseball_field/eval_set/f1_extra.PNG}
{include/graphs/learning/bedroom/eval_set/f1_extra.PNG}
{include/graphs/learning/bar/eval_set/f1_extra.PNG}
{The harmonic mean of recall and precision, F1-measure, read on the evaluation set over iterations. The performance of the first three settings peak early and then drops towards the end of the evaluation.}
{fig:learning:eval_set:f1}

The accuracy when using the different settings did not vary that much either. As seen in Figure \ref{fig:learning:eval_set:accuracy} the accuracy of the settings is around 85\% when classifying the category \emph{Bar} and around 95\% on the other two category evaluations. Just as with the F1-measure the accuracy measurements of the first three settings were higher in the first couple of iterations and then dropped off towards the end of the evaluation. 

\tripfigure
{include/graphs/learning/baseball_field/eval_set/accuracy_extra.PNG}
{include/graphs/learning/bedroom/eval_set/accuracy_extra.PNG}
{include/graphs/learning/bar/eval_set/accuracy_extra.PNG}
{The accuracy of the different settings on the three evaluation sets. Note how the accuracy of fourth setting varied more in-between evaluations than the accuracy of the other three settings did.}
{fig:learning:eval_set:accuracy}

\subsubsection{Search space}
\label{sec:res:learning:iter}
The different settings deviated a lot more from each other when comparing how the search space was classified compared to when the evaluation set was. The first three settings stopped predicting images as relevant early on in comparison with the fourth setting, resulting in the precision presented in Figure \ref{fig:learning:iteration:precision}. At the end of the evaluations the fourth setting had a precision of 15-25\% in all three categories while the other three settings could maintain a precision of about 80\% on the categories \emph{Bedroom} and \emph{Baseball field}. But when classifying the third category, \emph{Bar}, all the settings had a rather low precision. In this category, the first setting was the one showing the lowest precision. While the fourth setting had a relatively low precision, it did produce the best recall over the different search spaces as seen in Figure \ref{fig:learning:iteration:recall}. But when searching for the third category it took until the final iterations before the last relevant images were retrieved. To compare this with the other settings, where the last relevant image was retreived slightly after half of the evaluation. 

\tripfigurenear
{include/graphs/learning/baseball_field/iteration/precision_extra.PNG}
{include/graphs/learning/bedroom/iteration/precision_extra.PNG}
{include/graphs/learning/bar/iteration/precision_extra.PNG}
{The precision that the different settings had classifying the search spaces for the three evaluation categories. Note how the setting \emph{Top5+Bottom20} continued to predict images as positives when they indeed were negatives throughout the entire search.}
{fig:learning:iteration:precision}

\tripfigurenear
{include/graphs/learning/baseball_field/iteration/recall_extra.PNG}
{include/graphs/learning/bedroom/iteration/recall_extra.PNG}
{include/graphs/learning/bar/iteration/recall_extra.PNG}
{The recall rate on the three evaluation categories. The recall is slightly higher for those settings that continually present some of the \emph{bottom} images in each iteration.}
{fig:learning:iteration:recall}

In terms of performance as an image retrieval system the setting of presenting a majority of negatives is not to prefer. As seen in Figure \ref{fig:learning:iteration:retrieval}, the number of retrieved images by the fourth setting is a lot lower than the number for the other settings over the entire run. Looking at the performance on the \emph{Bar} category there are some iterations where the fourth setting has retrieved fewer relevant images than when selecting images at random. This is only momentarily and the rate soon returns to being slightly above that. Reading into the F1-measure in Figure \ref{fig:learning:iteration:f1} and the accuracy in Figure \ref{fig:learning:iteration:accuracy} the data is out of favour of the fourth setting. But when inspecting the readings of the \emph{Bar} category the \emph{Top20+Bottom5} setting has a slightly worse F1-measure and accuracy than the three other settings. 

\tripfigurenear
{include/graphs/learning/baseball_field/iteration/retrieved_extra.PNG}
{include/graphs/learning/bedroom/iteration/retrieved_extra.PNG}
{include/graphs/learning/bar/iteration/retrieved_extra.PNG}
{The number of retrieved images over iteration that the different settings had while classifying the search spaces for the three evaluation categories. Note how the \emph{Bar} category is more difficult than the other ones to retrieve relevant images from.}
{fig:learning:iteration:retrieval}
\tripfigurenear
{include/graphs/learning/baseball_field/iteration/f1_extra.PNG}
{include/graphs/learning/bedroom/iteration/f1_extra.PNG}
{include/graphs/learning/bar/iteration/f1_extra.PNG}
{The F1-measure that the different settings had classifying the search spaces for the three evaluation categories. An harmonic mean of the precision and the recall.}
{fig:learning:iteration:f1}


The setting that will be used to select the set of images each iteration needs to be picked in order to continue the parameter evaluation. When working with image retrieval the model needs to have a high retrieval rate of relevant images early on which causes the \emph{Top5+Bottom20} setting to not meet the preferences. To select between the remaining three settings one can recall what was mentioned in Chapter \ref{chapter:intro}: Investigation material needs to be retrieved in a quick manner and labeled correctly. In other words as few false negatives as possible. In all three categories; the \emph{Top20+Bottom5} setting had a higher recall rate on the search space throughout the evaluations (see Figure \ref{fig:learning:iteration:recall}). Which means that the first setting is the most appropriate one to use in the following benchmarks. 
\tripfigure
{include/graphs/learning/baseball_field/iteration/accuracy_extra.PNG}
{include/graphs/learning/bedroom/iteration/accuracy_extra.PNG}
{include/graphs/learning/bar/iteration/accuracy_extra.PNG}
{The accuracy that the different settings had classifying the search spaces for the three evaluation categories.}
{fig:learning:iteration:accuracy}
