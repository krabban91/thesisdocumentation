% !TEX root = ..\main.tex
\chapter{Image analysis theory}
\label{chapter:imagetheory}

This chapter introduces and explains how images can be represented in more ways than the rasterized formats that simply consists of the pixel data. The chapter introduces and dives into some of the different content representations that can be used as well as what features and feature descriptors are. The theory behind the feature descriptors within the scope of the thesis is explained. 

%\section{Image compression}
%\label{sec:image_comp}
%\todokahl{Behövs verkligen detta avsnitt?}
%\todo{Håller nog med Kahl. Finns bätre sätt att motivera resizing.}
%For regular images the size can be much larger and the time it takes to extract features becomes unrealistically for large datasets. The different ways in how one can compress the image have\todo{,} according to Juan M. Banda et al. \cite{banda2014image},\todo{something is missing, rephrase} the various techniques have all a low accuracy loss when compressing all the way down to JPEG 10\% which have 99.2\% space saving. The accuracy loss range from 0.1\% with no compression ratio change to 2.5\% for 1/100 compression ratio change. The paper make use of the SDO database, which is a dataset contributed by NASA. Based on this to be able to handle large data sets with no regard to the image quality all images are resized to be $256*256$ pixels in dimensions\todo{rephrase, be clearer}. Depending on the feature extractor at hand the images color data will also be changed to a corresponding color range or different string be omitted as e.g. the chroma\todo{luminance} strings in YCbCr in Sobel edge detector. 

\section{Visual features}
\label{sec:image_features}
Features in image processing are embedded information in either the picture itself or the meta-data concerning the picture. These features are extracted and used to solve various problems in computer vision, machine learning and pattern recognition. 
There are myriads of methods to extract features from an image and which one that suits the problem at hand varies. Commonly used features for CBIR are those that describe color, texture and shape. When the features have been extracted the resulting data are called feature vectors. The length of the feature vector varies, depending on the image being processed, the method used for description and the detail in which the features are extracted. 

\section{Feature detectors and descriptors}
\label{sec:feature_descriptors}
Feature detectors are usually built towards detecting either global features or local features. Global feature detectors are often color or texture oriented and makes a description based on all pixel data in the image. These are good at identifying similar images but can have a hard time to distinguish between foreground and background of an image as they work with the whole image and thus usually fail to find local nuances and differences. They are often built to output a feature vector with focus on a number of properties of the image involving all the pixels. In contrast the local feature detectors focus on keypoints in the image and try to describe these and sometimes the area of pixels around these. Local feature detectors are often used to locate and recognize identical objects which may be skewed and transformed in some way in different images. The result is often several vectors representing the points of interest in the image. These attributes come with a cost, local feature detectors are often expensive in terms of computational power and data storage \cite{hassaballah2016image}. \\

A Feature descriptor is used when the interest points have been identified, or detected as previously stated. The descriptor creates a set of vectors based on this information which can be used in the proposed retrieval model. In image processing and image analysis one uses feature descriptors to facilitate the transfer from an abundance of features that are derived from images to a subset or transformation of these features to create a more manageable dataset. Depending on the scope of the project the raw features one would get from the feature descriptors can become too large to handle and work with. To minimize this problem, the feature descriptors can be said to carry out dimensionality reduction on the data. Without any form of reduction of the actual information size there would be a large requirement on memory and computational power.

Feature descriptors can based on their performance also be divided into either weak learners or strong learners. A weak learner is a feature descriptor that have an average precision higher than random chance towards a certain category which. If the data were constructed of two classes with equally large categories a weak learner should have a precision just above 50\%. A strong learner in contrast is a feature descriptor with a much higher precision which on its own can discern a majority of the images, this can e.g. be a deep convolutional neural network. Note that there are not a clear distinction here since the classification of a feature descriptor as a weak or strong learner is not only which method that is used but which dataset it should be applied on. 


\subsection{Histogram of oriented gradients}
\label{sec:hog_feature}
Histogram of oriented gradients (HOG) is a feature descriptor used for object detection in the fields of image processing and computer vision. The idea being that a local object shape and appearance can be described by the distribution of intensity gradients, or edge directions. The HOG computes the first order gradient, as these capture contour and some texture information. This is all done on the locally dominant color channel which usually is the gray channel. Then a pooling method is used where the image is divided into a number of cells in where the gradient orientation is acquired. The orientations are distributed in a fixed number of bins, or possible orientations. The different magnitudes determine the result of the gradient histogram. When these has been evaluated the cells are grouped within blocks which creates a film over the ``surface'' of the cells of the image thus creating a new normalized gradient oriented image. The block are set to overlap thus each cell are accounted for several different calculations over different blocks. The normalized blocks are referred as HOG descriptors. The descriptor is essentially the concatenation of these local histograms. Examples of how the gradient orientations are binned can be seen in Figure \ref{fig:imagetheory:hog}. The HOG is well suited for human detection according to \cite{dalal2005histograms}. 

\fourfigure
{figure/hog/hog_gray.png}
{figure/hog/hog_extracted.png}
{figure/hog/hog_gray_cat.png}
{figure/hog/hog_extracted_cat.png}
{Examples of how the directions of gradients are binned in two images. The images are split up into cells with directional histograms. The number of histograms is substantially larger than when used in the thesis.}
{fig:imagetheory:hog} 

\subsection{Global color histogram}
\label{sec:gch_feature}

A color histogram is a representation of the color distribution in an image. The color histogram method counts the number of pixels with similar attributes and store them in a number of bins. There are no specific sizes for the bins, though there is a max size based on current color range used. The size used is dependent on the performance of using less bins in contrast to computational cost of using more bins. In image analysis it is common to use HSV for this histogram, which would result in three dimensions of bins, one for each color channel. In the Figure \ref{fig:imagetheory:gch} two examples of binnings are visualized, one with high and one with low resolution. When calculating a global color histogram (GCH) one does not take smaller patches of the image and calculate the concentration of information locally, instead it is calculated over the image as a whole. This approach can be insensitive where the objects might be similar but the color characteristics are. Two completely different images can still contain the same GCH values due to that the images have similar color settings.

\threefigure
{figure/gch/cat-bungy-256.png}
{figure/gch/all.png}
{figure/gch/our.png}
{The HSV color channels of the image to the left are binned into two histograms (H=blue, S=green and V=red). The left uses one bin per level of the color channels and the right histogram is grouped into a substantially smaller number of bins.}
{fig:imagetheory:gch} 

\subsection{Wavelet transform}
\label{sec:wlt_feature}

In this project a Haar Wavelet transform was implemented, the simplest of wavelets. Haar-like features are a type of digital image features which are used in object recognition \cite{pavani2010haar}. The wavelet was implemented by first taking the differences and means of each pixel to its neighbors. This is done once per pixel in horizontal calculations and once in vertical calculations, the values are then divided into the different rectangles. The signal is decomposed into a subset of signals representing different elements, approximations of the image and intensities in the different directions and orientations. So for each time the wavelet transforms the image a couple of smaller ones are created with the with a dimensionality of 2. This makes it possible for a wavelet with an image of size $256*256$ to be minimized 8 times ($2^8 = 256$). In Figure \ref{fig:imagetheory:wavelet} a representation of the wavelet transform for the three first levels is presented.

\fourfigure
{figure/WT0levels.png}
{figure/WT1levels.png}
{figure/WT2levels.png}
{figure/WT3levels.png}
{From left to right is shown representations of the different levels of wavelet transformation, original image, one level applied, two levels applied and three levels applied respectively.}
{fig:imagetheory:wavelet} 

%Is a time-frequency transformation often used for wavelet compression which is a \todo{Continue. Missing text.}

% Used to encode local appearance of objects which result in 2 or more rectangular regions enclosed into a template. 

% In our extractor 3 levels are used and as such, one get $1 + 9$ different values to create meta data from. These matrices (scalar for the first?) a set of bin strategies are used. in the different matrices are first absolute valued and summed as well as square rooted out the sum of the square of the absolute values minus the mu from the previous calculation. """" These results in rather few values so to create a larger search space a distribution is generated from these values to function as a larger set of similar values. """"

\subsection{Convolutional neural network activations}
\label{sec:vgg_feature}

The field of Artificial neural networks emerged with the McCulloch and Pitts neuron in 1943 \cite{mcculloch1943logical}. The idea was to simulate the human brain, where there are about $10^{11}$ nerve cells, or neurons, that through a symphony of signals communicate information from and to other neurons. This is achieved by creating nodes for the neurons and edges interconnecting the different neurons to simulate the axons and dendrites. Dendrites works as a form of input to the neurons which have different intensities in their signals, in artificial neural networks modeled using weights. The neuron, or soma, then sums up the inputs into an output which then the axon can signal forward to other neurons and so forth. From the basis of this simple adaptation the simulated neural network is created. In the simplest form a neural network is composed of a single layer of neurons. These are presented with an input with weights, which are analogous to the intensity of the signals measured frequency in the animalistic brain. The neurons then process these values into a response, or output.

\subsubsection{Convolutional neural networks}

In this thesis a subset of the field of artificial neural networks is used; convolutional neural networks (CNNs). A CNN is a type of a feedforward neural network  that tries to simulate the animal visual cortex. A feedforward neural network is an artificial neural network wherein the connections between the units do not form any cycles or loops. A multilayer feedforward network is composed by, an input layer, an output layer and zero or more ``hidden'' layers. In this simple illustration the first layer, the input layer, receives the first number of inputs to be processed by the neurons. this is then sent to the next layer as inputs and the chain continues until the final layer outputs the final result for the whole neural network. 

\subsubsection{Network components}
The architecture varies a lot between different designs where the number of layers, which type of layers as well as the parameters for the network can change drastically from network to network. The most common building blocks are convolutional layers, pooling layers and fully-connected layers. They are variations of multilayer perceptrons which are designed to use minimal amounts of preprocessing. In addition often used units are the rectifier linear unit, as well as some form of activation function. The CNNs are constructed by using several different components, some of the most commonly used ones are the following. \\

\textbf{\emph{The convolutional layer}} is the backbone of a CNN. They are composed of a set of learnable filters, aka kernels, with their distinct size of receptive fields. They traverse the whole area of the image, convolved across the height and width of the input, all the while computing the dot product. The combination of the filter locations produce a 2D activation map for each filter and each 2D activation map are stacked for each filter which creates the output matrix of the layer. \\
\textbf{\emph{The pooling layer}} is essentially a non-linear down-sampling of the input image. This is done by some algorithm where max pooling is one of the most commonly used ones. It splits the input into several smaller non-overlapping rectangles and then outputs the max value from these regions. The idea is to periodically insert pooling layers in between the convolutional layers, in so doing reducing the number of parameters as the size decreases and thus lessen the amount of computation needed for the network. This also helps to reduce the risk of overfitting. \\
\textbf{\emph{The rectifier linear unit}}, known as ReLU, which applies a non-saturating activation function to increase the nonlinear properties of the network. This is done without directly affecting the receptive fields of the convolutional layer. The general idea is to reduce the time it takes to train the network while still retain the generalizing nature of the network. It checks the values of the input layer and if the value is below 0 sets it to 0, and otherwise sets it to 1.\\
\textbf{\emph{The fully-connected layer}} is the last couple of layers in the CNN. The layers have full connection between the each other meaning that the activations in previous layers connects to all neurons in the next layer. These layers constitute the high-level reasoning of the network. \\
\textbf{\emph{The softmax activation function}} is normally used to produce the final output of the CNN where a loss function is set to determine how to penalize the network if prediction deviate from actual labeling. A softmax function is designed to use a probabilistic interpretation of the activation values and then normalize them. which very is useful when the output is applied to a cross-entropy loss.  \\

One of the hardest parts is to configure a CNN based on these layers to create a well versed and functioning network. In this thesis applies the VGG-16 \cite{simonyan2014very}. How the VGG-16 is built by using all these different layers in their convolutional neural network is shown to the left in Figure \ref{fig:imagetheory:vgg16}. Transfer learning is method where one use pretrained CNN models and then just remove the last output layer, and extract the features directly from the fully-connected layers \cite{koskela2014convolutional}. To be able to use the network for the purpose of a feature descriptor modifications were made. The the final fully-connected layer as well as the softmax layer, the loss layer, are both omitted as can be seen in Figure \ref{fig:imagetheory:vgg16}. 

\singlefigurenear
{figure/vgg_architecture.png}
{Top: A simplified visualization of the VGG-16 CNN. Bottom: The modification done in this thesis.}
{fig:imagetheory:vgg16}
{0.7}

\subsection{Edge detection histogram}
\label{sec:sob_feature}
The set of edge detectors is a group of different methods that aim to identify strong shifts in images, which could signify discontinuities in the image \cite{canny1986computational}. A common way is to use the channel representing light of a color space to find the discontinuities, like the luminescence (Y) as mentioned in Section \ref{sec:ycbcr}. The name edge detection is based on the relevant points that signify a discontinuity which are called edges. There are two distinct methods commonly used in edge detection which are the search based and the zero-crossing based. The first method, the search based, looks for the local directional maxima of the gradient magnitude, often using a first-order derivative expression to compute this. Examples of these are the Roberts, the Sobel  and the Prewitt operator, all edge detectors that utilize convolutional masks in order to calculate the gradient. Zero-crossing based, the other method, uses second order a derivative expression to find where there is a jump of values; the zero-crossing of the image. Example of these are the Laplacian operator, which often is used with an approximate convolutional mask \cite{jain1995machine}. The identification of these edges can be used to find more relevant objects and shapes in the images, which can be used to build predictions on \cite{maini2009study}. 

\twofigure
{figure/edge/Grey_cat.png}
{figure/edge/Sobel_cat_r.png}
{Edge detection, to the left a gray image of the original, to the right a image showing the edges found using Sobel edge detection}
{fig:imagetheory:edge} 
