% !TEX root = ..\..\main.tex

\section{Discussion of results}
\label{sec:concl:meas}
The performance of the model was best in the first half of a search during all parameter benchmark evaluations. This is noticeable when inspecting the learning method benchmark, Section \ref{sec:res:learning:eval}. If the training set that is acquired at the turnpoint -- when the model stops to predict images as relevant -- is persisted, would the performance of the model on the entire search then become better? The difference would probably not be noticeable when inspecting the measurements on the the search space since the model has already started to predict the rest of the search space as irrelevant images. But this would however mean that training set makes the model perform well on the independent evaluation set and the rest of the search space can be predicted as non-relevants without taking up additional computational power. In every evaluation there has however always existed some outliers that were found after the turnpoint described earlier but in most cases became false negatives. Our iterative model could in other words not prevent this from happening. 
In the same section, Section \ref{sec:res:learning}, one setting had a lower count of false negatives; the setting called \emph{Bottom20+Top5}. When evaluating the performance on the search space the recall for this method was higher than for the other ones. This would reduce the risk of any false negatives, but using this setting would result in not reaching the turnpoint mentioned earlier due to positive predictions until the end of the evaluation. Meaning that the matching module has to keep on adapting to the given data. On top of that the rate of retrieving relevant images that this setting will always be lower than for the other settings, which makes it less of an appropriate setting for image retrieval. 

To search the entire search space, as the model in evaluation in Section \ref{sec:res:learning} did, was never intended. But since the focus of the thesis was to create a lightweight iterative model that can retrieve images, optimization became a second priority. Then again, the intention was to model something that reduced time spent per image so the two stopping 
conditions were applied. Since the combined pruning of the search space barely affected how the pace of how the matching module learned, more work could be done to reduce the time spent. To see what would happen to the performance (as well as the total time taken) if the classifier in the matching module would, as mentioned earlier, stop adapting its decision boundary to data after a certain point would be interresting. In Figure \ref{fig:stopping:eval_set:f1} this turnpoint is around iteration 20 and 40 depending on the category. And inspection the time taken seen in Figure \ref{fig:stopping:iteration:totaltime} this could mean a reduction from 0.5 of the total to $\approx0.05$ of the total time taken.  

Due to that when only using the activation vector from a CNN as a descriptor resulted in time reduction of about 50\% compared to when using the complete set of different descriptors as input to the deep SVM, it might seem tedious to use the full set of descriptors when the performance were about the same for the two settings. But it would however be interesting to see how well the model performs if one would combine the activation vectors from multiple networks that have been trained for different purposes. The measurements presented in Section \ref{sec:res:features} do in fact indicate that there are some improvements when using an ensemble of classifiers compared to only using its parts. Especially in Figure \ref{fig:features:eval_set:f1_no_cnn} where the the performance was noticeably increased by using the feature descriptors in unison. 

In the final parameter benchmark, the training set evaluation in Section \ref{sec:res:knownset}, it became clear that if the refitting of the classifier was omitted the time taken was drastically reduced (see Figure \ref{fig:knownset:iteration:timetaken}). But if the model only used data that is retrieved from relevance feedback, the model would perform equally wel at the evaluation set as a model with a predefined training set with 250 relevant and 250 irrelevant images (see Figure \ref{fig:knownset:evalset:f1}). Could this mean that after a number of iterations the proposed model could be used to find a subset of a category that defines the essence of the category? I.e. removing the outliers from a category that, in terms of the currently selected feature descriptors, lies closer to another category. This could be used to not only fine-tune a training set intended for some specific image retrieval, but also to manifest as the semantic gap between human and machine learning. The outliers that are not included in such a search defines what either the model could not see or what the person has put in a category that does not fit the description by mistake.

The productivity of the proposed model was also shown in the evaluation that was performed on the Corel-1000 set in Section \ref{sec:res:studycomp}. Even though the model is designed to improve the training set in an iterative manner, it behaved surprisingly well with small query sets. This procedure was never tested on the same dataset that the benchmarks performed on but doing so might give some interesting results. It was interesting to see that the proposed model could produce an similar precision as the neural network model proposed in \cite{elalami2014new} when having a query set of 5 relevant and 5 irrelevant images while the model described in that paper uses a training set of 900 images. However, the evaluation was not fair to all parts. The model, that uses an ANN to categorize material, uses a test set of 100 images while the test set used for the proposed model was substantially larger. Having a small query set allowed the deep SVM to fit immediatly and classifying the a dataset is done rather quick compared to calculating individual distances. Just as having more than one image as a query allows the model to ignore outliers and can still find images that are defined as matches. Outliers that the papers \cite{wang2001simplicity}, \cite{subrahmanyam2013modified} and \cite{nagaraja2015low} have to include in their results. This evaluation will however give future studies the possibility to compare with a multi image query-based CBIR approach on the Corel-1000 set.
