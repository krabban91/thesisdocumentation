% !TEX root = ..\..\main.tex
\section{Future work}
\label{sec:concl:future}
Some of the possible improvements might have been mentioned previously in this thesis, but in this section all future work is listed and discussed.
The different ideas of improvements is\todokahl{are} split up into two parts; how the model can be improved \todo{as well as}and possible usage areas that were not intended initially.

\subsection{Model improvements}
\label{sec:conc:future:model}
The different model improvements included in this section are time investments that could have been done within a \todo{a near future but} short time period but were omitted since they were not covered by the focus of the thesis.

\subsubsection{Scaling to bigger datasets}
\label{sec:conc:future:model:data}

A future feature for the system would be to create a search algorithm that improves the way images are selected for each iteration based on previous predictions.
As of now the procedure to select images is to randomly sample a predefined set size from the search space. 
Problem\todo{problems arise} arises as \todo{the datasets become}these dataset becomes larger,\todo{``, and''=> ``. Some form''} and some form of sorting can minimize the time spent searching for the relevant images as well as improve the rate in which they are located. 
The algorithm can for example, either sort the material after how relevant it was predicted \todo{to be}as in previous iterations or in how the folder structure of the database is composed. This would thus minimizing the size of the \todo{viable }search space and allow larger parts of the unknown set to be evaluated faster as well as the material that is located in the same folders is ensured to be examined at some point.

\subsubsection{Improvements to the relevance feedback loop}
\label{sec:conc:future:model:rf}
The number of images presented to the user in each iterations is in the proposed model set to \todo{In the proposed model the number of images predicted as relevant and irrelevant is set to 20 and 5 respectively. }20 images perceived as relevant and 5 as non-relevant. This is in no way based on previous studies or knowledge and is set after what \todo{add ``empirically'' somewhere}\emph{felt right} to the authors after some testing. To optimize the performance of the algorithm in conjunction with the user, considerations should be made in how many images should be presented each iteration. The number of images presented might be decreased of increased, depending on empirical\todo{emperical =>``result of studies''} studies, to enhance the performance and \todo{improve}enhance the user experience.

In the proposed model the relevance feedback module, described in Section \ref{sec:method:proposed:rf}, only updates the search space with which images that are relevant and which are not\todo{when=> ``. This when knowning''} when knowing exactly which images that were mistakenly perceived as relevant and non-relevant by the matching module. If this information was used it would be possible to ``move'' the decision boundary. 
One example is that it is possible to weight each datapoint\todo{data point} in order to make them more decisive\todo{impactful} in how the classifier makes decisions. If an image is falsely categorized as a non-relevant one, it could be inserted into the dataset as \todo{emph}two \todo{data points instead of one}points. If the \todo{remove emph}\emph{two} data points are close to a decision boundary this would increase the cost of ending a training session at this point. By putting more energy into using relevance feedback \todo{remove ``more heavily''}more heavily, the semantic gap between man and machine might be reduced. 

As \todo{remove ``can be''}can be seen in the results the \todo{the performance of the model}models performance will decrease as the \todo{number of relevant images in the search space becomes exhausted.}dataset is exhausted of relevant images. As this is a controlled environment this can not be \todo{proven for untested datasets}taken as true for unknown datasets. If presented to unknown data, that the algorithm does not find any relevant\todo{material after a couple} in a couple of iterations it does not mean that relevant images are exhausted. There is however much to gain with cutting the retraining short as mentioned in Section \ref{sec:concl:meas}. To stop the retraining and assume \todo{that }all remaining images are non-relevant when the algorithm only finds non-relevant images \todo{seems to be the approach that is more time-efficient and maintains the performance of the model}is both time efficient and help to keep the performance. There need to be some feature that activates the retraining again if the user were to label images as relevant again as well. 

\subsubsection{Selection of feature descriptors}
\label{sec:conc:future:model:feat}
In this thesis, not too much effort was put into selecting the best suited feature descriptors since the case is of the general nature and instead \todo{feature descriptors were chosen}chose feature descriptors \todo{based on}after the idea that in order to create a more general classifier\todo{add ``,''} more feature descriptors would be added. There are however feature descriptors that perform better or worse in these situations.

In the results\todo{of the feature descriptor benchmark}, Section \ref{sec:res:features}, the \todo{setting that uses the }CNN activation vector is seen outperforming the \todo{settings that use the }other feature descriptors by a large margin, but \todo{the margin is far more narrow when presented}can be narrowly seen to receive some performance improvement when presented towards the \emph{bar} category. Instead of mixing weak and strong learners,\todo{. One could for example combine} one could combine the activations of the final fully connected layers of several neural networks \todo{that have been trained}which were trained towards different datasets \todo{remove ``and purpose''}and purpose into an ensemble. The idea being to cover as many possible features as possible within the different networks. 

In \todo{In the}The proposed model, all classifiers are always present and their predictions are always taken into account. If there would be cases where some of these \todo{classifiers}classifier do not contribute or even might reduce performance by misclassifying, it might be beneficial to shut them \todo{down}off. During the parameter benchmark\todo{, in Section \ref{sec:res:features}, } some of the feature descriptors were found lacking in performance on certain sets. The improvement being to reduce time calculating \todo{remove ``information''}information that is of no use for the matching module by terminating certain parts of the classifier.

\subsection{Miscellaneous usage areas}
\label{sec:conc:future:misc}
Besides from improving the proposed model by adding different features there are some possible usage areas outside of the domain of the thesis that were considered. These ideas are briefly described \todo{in this section.}here.
\subsubsection{Dataset improvement}
\label{sec:conc:future:misc:data}

As mentioned in Section \ref{sec:concl:meas} the images that represent the essential bits of a concept are selected early on during a search space exploration and the outliers that are selected later on seem ruin classification results. This information could be used to design a smaller training set specialized for a certain cause. 
In the scenario of a system that often uses a large dataset in order to solve some machine learning problem or an image retrieval problem, the feature \todo{previously described}found in this system could be applied. \todo{the following two sentences are F-d.}The possible functionality of the classifier could stop refitting the decision boundary after a point, described in Section \ref{sec:conc:future:model:rf}, could be used to improve an already existing dataset. If the training set that achieves the best results is extracted it might perform just as well as the original large dataset.
