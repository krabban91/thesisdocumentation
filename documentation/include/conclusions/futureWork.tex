% !TEX root = ..\..\main.tex
\section{Future work}
\label{sec:concl:future}
Some of the possible improvements might have been mentioned previously in this thesis, but in this section all future work is listed and discussed.
The different ideas of improvements are split up into two parts; how the model can be improved as well as possible usage areas that were not intended initially.

\subsection{Model improvements}
\label{sec:conc:future:model}
The different model improvements included in this section are time investments that could have been done within a near future but were omitted since they were not covered by the focus of the thesis.

\subsubsection{Scaling to bigger datasets}
\label{sec:conc:future:model:data}

A future feature for the system would be to create a search algorithm that improves the way images are selected for each iteration based on previous predictions.
As of now the procedure to select images is to randomly sample a predefined set size from the search space. 
Problems arise as the datasets become larger. Some form of sorting can minimize the time spent searching for the relevant images as well as improve the rate in which they are located. 
The algorithm can for example, either sort the material after how relevant it was predicted to be in previous iterations or in how the folder structure of the database is composed. This would thus reduce the size of the viable search space and allow larger parts of the unknown set to be evaluated faster as well as the material that is located in the same folders is ensured to be examined at some point.

\subsubsection{Improvements to the relevance feedback loop}
\label{sec:conc:future:model:rf}
In the proposed model the number of images predicted as relevant and irrelevant each iteration is set to 20 and 5 respectively. This is in no way based on previous studies or knowledge and was set based on empirical testing and what \emph{felt right} to the authors. To optimize the performance of the algorithm in conjunction with the user, considerations should be made in how many images should be presented each iteration. The number of images presented might be decreased of increased, depending on result of studies, to enhance the performance and improve the user experience.

In the proposed model the relevance feedback module, described in Section \ref{sec:method:proposed:rf}, only updates the search space with which images that are relevant and which are not. This when knowing exactly which images that were mistakenly perceived as relevant and non-relevant by the matching module. If this information was used it would be possible to ``move'' the decision boundary. 
One example is that it is possible to weight each data point in order to make them more impactful in how the classifier makes decisions. If an image is falsely categorized as a non-relevant one, it could be inserted into the dataset as two data points instead of one. If the two data points are close to a decision boundary this would increase the cost of ending a training session at this point. 
%By using more data from relevance feedback, the semantic gap between man and machine might be reduced. 

As seen in the results the performance of the model will decrease as the number of relevant images in the search space becomes exhausted. As this is a controlled environment this can not be proven for untested datasets. If presented to unknown data, that the algorithm does not find any relevant material after a couple of iterations it does not mean that relevant images are exhausted. There is however much to gain with cutting the retraining short as mentioned in Section \ref{sec:concl:meas}. To stop the retraining and assume that all remaining images are non-relevant when the algorithm only finds non-relevant images seems to be the approach that is more time-efficient and maintains the performance of the model.

\subsubsection{Selection of feature descriptors}
\label{sec:conc:future:model:feat}
In this thesis, not too much effort was put into selecting the best suited feature descriptors since the case is of the general nature and instead feature descriptors were chosen based on the idea that in order to create a more general classifier, more feature descriptors would be added. There are however feature descriptors that perform better or worse in these situations.

In the results of the feature descriptor benchmark, Section \ref{sec:res:features}, the setting that uses the CNN activation vector is seen outperforming the settings that use on of the other feature descriptors by a large margin. Even if the margin is far more narrow when presented towards the category Bar. Instead of mixing weak and strong learners. One could for example combine the activations of the final fully connected layers of several neural networks that have been trained towards different datasets and insert them into an ensemble. The idea being to cover as many possible features as possible within the different networks. 

In the proposed model, all classifiers are always present and their predictions are always taken into account. If there would be cases where some of these classifiers do not contribute or even might reduce performance by misclassifying, it might be beneficial to shut them down. During the parameter benchmark, in Section \ref{sec:res:features}, some of the feature descriptors were found lacking in performance on certain sets. The improvement being to reduce time calculating that is of no use for the matching module by terminating certain parts of the classifier.

\subsection{Miscellaneous usage areas}
\label{sec:conc:future:misc}
Besides from improving the proposed model by adding different features there are some possible usage areas outside of the domain of the thesis that were considered. 
\subsubsection{Dataset improvement}
\label{sec:conc:future:misc:data}

As mentioned in Section \ref{sec:concl:meas} the images that represent the essential bits of a concept are selected early on during a search space exploration and the outliers that are selected later on seem ruin classification results. This information could be used to design a smaller training set specialized for a certain cause. 
In the scenario of a system that often uses a large dataset in order to solve some machine learning problem or an image retrieval problem, the feature previously described could be applied.
To stop a search at the point where the classifier performs the best, as described in Section \ref{sec:conc:future:model:rf}, could result in receiving a smaller dataset that can train a classifier faster for the intended purpose and the classifier maintains its performance. 
