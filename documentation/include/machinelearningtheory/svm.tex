% !TEX root = ..\..\main.tex
\section{Support vector machines}
\label{sec:mltheory:svm}

\todo{somewhere in this paragraph: Insert that it is a geometrical comparison.}A Support vector machine (SVM) is essentially a supervised learning model where data is analyzed for classification and regression analysis. Can be said to be a non-probabilistic \todo{does this sentence explain non-probabilistac-ness?}binary classifier since new examples are assigned to either of two categories\todo{rephrase}. The SVM model is a representation of the examples\todo{of the examples?} as parts in space that are mapped in a way, as clear as possible, that separates the classes by a margin. A data point of a set is considered as a p-dimensional vector (composed of p numbers) and the goal is to be able to separate the data with a (p-1)-dimensional hyperplane. 

SVMs have been found to function well on both small and large numbers\todo{size} of dimensions \cite{vert2005kernel}, but all datasets are not easily divided by a linear model.  Due to this the \emph{Kernel trick} was invented. A non-linear classification implicitly mapping their inputs into a different dimensional feature space.

SVMs can be used for classification, regression and outlier detection. But since the thesis has its focus within classification the theory covering the other two use cases will be omitted. 

In order to classify an SVM constructs a hyperplane, or a set of hyperplanes in higher dimensional spaces, that can be used to classify data points depending on which side of the hyperplane they reside. \todo{rephrase}Due to the limitation the number of sides that exist of an hyperplane this becomes a binary classification and therefore the label set becomes binarily defined as $\mathbf{Y} \in \{1,-1\}$. The optimal hyperplane $\vec{w}^T\vec{x}+b = 0$, as seen in Figure \ref{fig:svm_margin}, is found when the given training data is separated with an as large margin $\gamma = \frac{1}{||{\vec{w}||}}$ as possible. Which means that it will also be found when minimizing $||\vec{w}||$, \todo{and instead of all else}as well as when minimizing $\vec{w}^T\vec{w}$. 
Given training data points, or vectors, $\vec{x_i} \in \mathbb{R}^p, i=1...n$ and the respective label $y_i \in \mathbf{Y}$ the primal (\ref{eq:svm_primal})
\begin{equation}
\label{eq:svm_primal}
\begin{split}
\min_{\vec{w}\in\mathbb{R}^p,b\in\mathbb{R}}{\vec{w}^T\vec{w}}& \\
\textnormal{subject to} & \\
\forall j\textnormal{: } (\vec{w}^T\vec{x}_j + b)y_j & \geq 1
\end{split}
\end{equation}
can be constructed.

As soon as a stable hyperplane has been found the test set can be classified by simply checking which side, of the hyperplane, the data points end up on by computing the label value (\ref{eq:svm_test})
\begin{equation}
\label{eq:svm_test}
y_i = sign\left(\mathbf{w}^T\mathbf{x}_i+b\right).
\end{equation}
The larger the distance from the hyperplane to a point the more certain the prediction is that the data point belongs to a certain category. Hence the data points that are within then margin of the hyperplane have the most uncertain predictions. This can in fact be used in order to calculate some certainty that a data point belongs to a class or not. If the distance between two data points and the decision boundary compared is of different sizes, the point with the greater distance is more probable to be of the desired category \cite{tong2001support}\todo{rephrase}. 


\singlefigure
{figure/SVM_margins.png}
{A simplified visualization of how data is linearly separable in a two dimensional space.}
{fig:svm_margin}
{1}


\subsection{Kernels}

Kernels define the cartesian product between vectors, which can be used to get the a real value. In order to create a kernel one defines a function $K$ from the cartesian product of the feature space to a real value ($K:\mathcal{X}\times \mathcal{X} \rightarrow \mathbb{R}$). This real value can subsequently be used to evaluate a distance value. Depending on the chosen kernel the distance have different attributes an thus the kernel can be selected to receive better results for different datasets.
Kernels in SVMs are different methods of how the hyperplane is generated for the SVM and thus gives different ways of separating the data.  The most common ones are the \emph{linear} and the \emph{radial-basis function} (RBF) kernels. The linear kernel is a straightforward approach which is as the name suggests a separates data in in a linear manner. 
But when data data is not linearly separable this kernel will not suffice. 
An example of a kernel that could solve solve this problem is the RBF. The RBF kernel is a fast approach that often work, as long as the feature space is not too large, and can be used to separate inliers from outliers. The kernel often uses a norm function between two points in order to separate them, e.g. the squared two-norm  (\ref{eq:rbf_kernel})
\begin{equation}
\label{eq:rbf_kernel}
K_{RBF}(\textbf{x},\textbf{y}) = ||\textbf{x}-\textbf{y}||_2 ^2=\sum_{i=1}^p  (\textbf{x}_i-\textbf{y}_i)^2.
\end{equation}
Different kernels are used to make data points linearly separable in their own dimensional spaces, causing the decision boundary in the original feature space to be, and appear, non-linear.
